# Patient-Side Architecture: Structured Risk Profiling Layer

## Design Principle

Do NOT allow free-text inputs like:

> â€œI have diabetes and sometimes go to hospital.â€

Free text:
- Is vague
- Is hard to evaluate
- Makes filtering nondeterministic
- Pushes too much responsibility to the LLM

Instead, build a structured intake system.

---

# ğŸ— Step 1 â€” Patient Profile Schema

## A. Basic Demographics

- Age
- Employment status
- Monthly insurance budget
- Family coverage required? (Y/N)

---

## B. Medical Profile

- Chronic conditions (multi-select)
- Medication frequency (daily / weekly / monthly)
- GP visits per year (numeric)
- Specialist visits per year
- Hospital admissions (last 2 years)
- Planned procedures? (Y/N)

---

## C. Risk Sensitivity

- Preference: low premium vs high coverage
- Risk tolerance level (1â€“5 scale)

---

## Why This Matters

Structured inputs create:

- Evaluatable features
- Deterministic filtering logic
- Clean architecture separation
- Reproducible experiments

This directly strengthens your evaluation section.

---

# ğŸ¯ Step 2 â€” Risk Scoring Module (Non-LLM Layer)

Do NOT let the LLM â€œinferâ€ risk.

Instead, compute a structured risk score.

## Example Risk Formula

```
risk_score =
  chronic_condition_weight +
  medication_frequency_weight +
  hospital_visit_weight +
  age_weight
```

You can define risk tiers:

- Low Risk
- Medium Risk
- High Risk

This converts:

Structured patient â†’ Risk vector â†’ Plan filtering

This demonstrates system design maturity.

---

# ğŸ” Step 3 â€” Matching Engine (Pre-RAG Filtering)

Before calling RAG:

Filter out plans that:

- Exclude pre-existing conditions
- Have insufficient outpatient coverage
- Exceed budget constraints

Then rank remaining plans by:

- Cost-to-coverage ratio
- Risk coverage adequacy score
- Waiting period penalties

This stage is deterministic.

LLM is NOT involved here.

---

# ğŸ§¾ Step 4 â€” RAG for Justification (Post-Selection)

Only after selecting top 2â€“3 candidate plans:

1. Retrieve relevant policy clauses
2. Generate grounded explanation

The LLM is responsible for:

- Explanation
- Clause citation
- Transparency
- Personalized summary

It is NOT responsible for:
- Budget filtering
- Risk scoring
- Eligibility logic

This separation is critical for grading.

---

# ğŸ›¡ Step 5 â€” Safety + Human-in-the-Loop

Because this is healthcare-related:

Include:

- Decision-support disclaimer banner
- Confidence score output
- â€œRequest Expert Reviewâ€ button
- Logging of override decisions

In the report:

> â€œThe system maintains human oversight for high-risk recommendations.â€

This signals responsible AI deployment.

---

# ğŸ“Š Evaluation Benefits

This layered design enables structured evaluation.

## 1ï¸âƒ£ Recommendation Quality

- Budget compliance rate
- Risk threshold satisfaction
- Coverage adequacy score

---

## 2ï¸âƒ£ Explanation Faithfulness

- Are retrieved clauses cited?
- Hallucination rate
- Alignment between retrieval and explanation

---

## 3ï¸âƒ£ Edge Case Testing

Create synthetic patient profiles:

- High-risk chronic condition
- Low-risk healthy adult
- Budget-constrained user
- Maternity case
- Specialist-heavy case

Evaluate system consistency across scenarios.

---

# ğŸ§¬ Optional: Large-Scale Synthetic Testing

Generate 50â€“100 synthetic patient profiles.

Evaluate:

- Stability of recommendations
- Retrieval accuracy per condition category
- Distribution of risk tiers
- Explanation consistency

This transforms testing into systematic evaluation.

---

# âš  Core Design Principle

Patient data should NOT be fed directly into RAG.

Correct pipeline:

Patient â†’ Structured Risk Layer â†’ Plan Shortlist  
THEN  
RAG â†’ Retrieve Clauses â†’ Explain

This layered design should be clearly illustrated in your system architecture diagram.

---

# ğŸ“ Final Architecture Flow

User Input (Structured Form)  
â†“  
Risk Scoring Module  
â†“  
Plan Filtering & Ranking  
â†“  
RAG (Policy Retrieval)  
â†“  
LLM Explanation  
â†“  
Evaluation + Logging  

This is a proper ML deployment system â€” not a chatbot demo.
